### 🔁 백업 흐름 (클라이언트 중심)

1. **클라이언트**에서 백업 유틸리티 실행 (with `ProjectId`, `대상 폴더`)
2. **서버**에 `ProjectId`로 가장 최신 Commit 요청
3. **서버 응답**: Commit에 포함된 전체 파일 metadata (full path, mtime, size)
4. **클라이언트에서 현재 파일 시스템 상태와 비교**
   - 변경된 파일 (mtime/size 다름)
   - 추가된 파일 (경로 없음)
5. **변경/추가 파일들과 그 metadata**를 tar.gz로 압축
6. 서버에 전송
7. **서버는 metadata + hash 비교로 중복 제거 후 DB 저장**

---

## ✅ 이 구조의 장점

### 1. 🧠 **클라이언트가 더 많은 문맥을 알고 있음**
- 전체 파일 시스템을 직접 접근 가능 → `mtime`, `size`, `existence` 쉽게 확인
- 변경된 파일 여부를 **서버가 추론할 필요 없음** → 서버는 단순 수신처리만 하면 됨

### 2. 🚀 **성능 최적화가 쉬움 (Rust 활용)**
- 클라이언트는 멀티스레드로 파일 탐색, 메타데이터 추출, 해시 계산을 병렬 처리 가능
- Rust의 `rayon`, `walkdir`, `flate2` 등 사용 시 고속 처리

### 3. ⚙️ **구조가 단순해짐 (비즈니스 로직 분리)**
- 서버는 dumb storage 역할에 집중 (metadata 저장, 중복 제거, 응답)
- 복잡한 변경 감지/비교 로직은 클라이언트에 있음 → **역할 분리로 유지보수 쉬움**

### 4. 🧪 **테스트 및 디버깅 용이**
- 서버에 문제 생겼을 때 클라이언트가 보낸 것만 보면 됨 (tar.gz + metadata)
- 클라이언트는 테스트시 로컬에서 metadata diff만 해도 동작 확인 가능

### 5. 💡 **증분 백업 구현이 명확해짐**
- 클라이언트가 직전 상태를 명확히 알고 있으므로 어떤 파일이 변화했는지 정확히 판단 가능
- 서버는 수동 비교, 재요청, 커밋 상태 비교 등을 하지 않아도 됨

---

## 🧱 시스템 구조 비교

| 항목 | 서버 중심 방식 | 클라이언트 중심 방식 (제안한 방식) |
|------|----------------|----------------------------------|
| 변경 파일 판단 | 서버가 추론 | 클라이언트가 정확히 판단 |
| 전송 데이터 | 변경된지 모르는 전체 파일/시간 기반 추정 | 변경된 파일과 정확한 metadata |
| 서버 부담 | 비교 로직, 재요청 등 로직 복잡 | 수신 및 저장 중심 |
| 동기화 정확도 | mtime 누락 가능성 있음 | full metadata diff로 정확 |
| 확장성 | 서버 쪽 부하가 집중 | 클라이언트 분산 처리 |
| 코드 구조 | 로직 분산, 상태 관리 복잡 | 역할 명확히 분리됨 |
| 추후 기능 확장 | 어렵고 충돌 우려 | 유연한 버전 관리 및 기능 확장 용이 |

---

## 🔧 설계 팁

1. 클라이언트에서 metadata 추출 시:
   - full path, size, mtime 외에 선택적으로 `partial hash`를 추가해도 좋음
2. 서버 응답의 commit metadata는 gzip 등으로 압축해서 내려도 무방 (전송 최적화)
3. metadata는 정렬해서 보내면 서버에서도 중복 비교/기록이 더 간편
4. 서버에서는 동일 파일은 `hash`로 deduplication, 동일 metadata는 버전 관리 형태로 저장

---

## ✅ 결론

> 지금 제안한 구조는 **더 단순하고, 신뢰성 있고, 확장성 있는 백업 구조입니다.**

- 클라이언트가 상태 판단을 책임지고
- 서버는 수신과 저장에 집중하는 방식은
- 실제 백업 솔루션 (예: restic, borg, duplicity 등)에서도 사용하는 구조입니다.

---


``` mermaid
flowchart TD
    subgraph Client
        A1[백업 프로그램 시작<br/>ProjectId와 폴더 정보 서버로 전송]
        A2[서버로부터 최근 Commit의<br/>전체 파일 metadata 수신]
        A3[로컬 폴더 스캔하여 현재 metadata 추출]
        A4[서버 metadata와 비교하여<br/>변경 및 추가된 파일 목록 생성]
        A5[변경/추가 파일과 metadata를<br/>tar.gz로 압축]
        A6[tar.gz와 metadata를 서버에 전송]
    end

    subgraph Server
        B1[tar.gz 및 metadata 수신]
        B2[파일 해시 계산 및 중복 제거]
        B3[새로운 파일과 metadata DB에 저장]
        B4[Commit 정보 업데이트]
    end

    A1 --> A2 --> A3 --> A4 --> A5 --> A6 --> B1 --> B2 --> B3 --> B4


```

# ✅ 클라이언트 중심 백업 구조 - NAS, 빌드 후 자동 백업 포함 예외 및 주의사항

---

## 📌 상황 요약

- **NAS에 마운트된 디렉토리에서 빌드**
- 빌드 완료 후, 백업 프로그램이 자동 실행됨 (`make` 완료 후 후처리로 실행)
- 백업 대상은 `projectId + 폴더`
- 백업 시 전체 metadata와 변경된 파일만 전송
- 서버는 파일 중복을 해시로 제거하고, metadata를 DB에 저장

---

## 🧠 주요 예외/주의사항 정리

---

## 1. NAS 파일시스템 특이사항

### 1.1 mtime 정확도/일관성 이슈
- 문제: NAS는 로컬 FS와 달리 mtime 정밀도가 낮거나 갱신 지연이 있을 수 있음 (특히 NFS, SMB)
- 대응:
  - `mtime` 비교 전 `sleep` 100~500ms 정도 지연
  - `mtime`, `size`뿐 아니라 필요 시 `partial hash`도 병행
  - 테스트 환경에서 NAS의 `stat` 응답 확인 권장

### 1.2 `make`와 NAS 캐시 동기화
- 문제: 빌드 후 파일이 NAS에 실제 반영되기 전 백업이 시작될 수 있음
- 대응:
  - `make` 종료 후 백업 전에 `sync` 또는 `fsync` 강제 호출
  - NAS 옵션 중 `noac`, `actimeo=0` 등 캐시 정책도 점검

---

## 2. 빌드 직후 자동 실행의 주의점

### 2.1 백업 프로그램이 빌드 도중 실행될 가능성
- 문제: 병렬 빌드(make -j) 중 `all`이 완료되었지만 일부 출력이 아직 진행 중일 수 있음
- 대응:
  - `.PHONY`로 백업 명시하고, 백업 전 `sleep` 또는 파일 안정성 검사 추가
  - 예시: 파일 size가 일정 시간 변화 없을 때만 백업 대상 포함

### 2.2 파일 권한 / 임시 파일
- 문제: 빌드 중 생성된 임시 파일이나 권한 없는 파일이 tar.gz에 포함될 수 있음
- 대응:
  - `.gitignore`나 `.backupignore` 기반으로 필터링
  - 빌드 산출물만 포함되도록 `.backup_include` 설정 가능

---

## 3. metadata 정확성 및 일관성

### 3.1 이름 변경 / 삭제 탐지 어려움
- 대응:
  - 서버에 이전 커밋 metadata가 있기 때문에, 클라이언트에서 metadata 비교 시 “없어진 파일”도 감지 가능
  - 백업에 삭제 리스트를 포함하거나 서버 측에서 비연속 검출 가능

### 3.2 mtime 동일 but 내용 다름
- 대응:
  - 빌드시 `touch`로 mtime이 유지되는 경우를 대비해 변경 감지 fallback → `size`, `mtime`, `partial hash`

---

## 4. 전송 및 무결성

### 4.1 NAS에서 압축시 IO 속도 저하
- 문제: NAS에서 직접 읽어서 tar.gz 생성하면 속도가 느림
- 대응:
  - 백업 대상 파일을 로컬 temp 디렉토리로 copy 후 tar.gz 압축
  - 또는 tar 중에 `BufReader` / `BufWriter` 사용으로 IO 최적화

### 4.2 압축 중간 오류 시 불완전 전송
- 대응:
  - tar.gz 자체 checksum 포함
  - 압축된 tar.gz의 파일 목록 log 추출 및 검증

---

## 5. 권한, 보안

### 5.1 NAS 파일의 접근 권한
- 문제: 일부 빌드 생성 파일이 root 권한 등으로 읽기 불가할 수 있음
- 대응:
  - Rust에서 `entry.metadata().permissions()`로 사전 확인
  - 접근 불가 파일은 누락되었음을 로그로 알림

### 5.2 인증 및 projectId 매칭 오류
- 대응:
  - 백업 시 projectId + 디렉토리 path를 서버에서 유효성 검증
  - 클라이언트 인증 토큰 사용 권장

---

## 6. 성능 및 병렬화

### 6.1 NAS는 IO 병목이 심함
- 대응:
  - WalkDir + Rayon으로 병렬 metadata 수집
  - tar.gz 생성도 파일 스트림을 병렬로 처리 (비동기 가능)

### 6.2 전체 metadata 크기 증가
- 대응:
  - metadata JSON 압축
  - 변경된 파일만 metadata 추출하는 옵션 분기 고려

---

## ✅ 정리

- **NAS 환경**은 캐시, mtime, IO 성능 등에서 로컬과 매우 다르므로 보정 로직 필요
- **빌드 후 백업 자동화**는 유용하지만 안정성 확보가 핵심 (시간 지연, 확인 단계 삽입 권장)
- `mtime+size` 만으로 변경 감지는 충분하지 않을 수 있음 → `partial hash` 또는 content-aware 대응 필요
- 전송/압축 실패나 파일 권한 오류 대비를 위한 방어 코드도 중요

---

💡 필요 시:
- `.backupignore`, `.backupmeta` 같은 구조 제안 가능
- NAS 최적화된 압축/전송 방식 예시 추가 가능
- 실제 `Makefile`에 삽입하는 백업 명령도 제공 가능
