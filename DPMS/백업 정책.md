# 반도체 장비 백업/버전 관리 최적 방안 (현재 인프라 기준)

---

## 1️⃣ 현재 인프라

- **장비 12대**
  - 로컬 디스크 2개 (1번 → 2번 하루 1회 백업)
  - 저장 데이터: pgm, 빌드 바이너리, 결과파일 (대략 300GB/장비)

- **서버**
  - 리눅스 4TB (장비 마운트 가능) → 실행/빌드 안정적, 용량 제한
  - 윈도우 서버 2대, 80TB RAID5, HA 구성, 1시간 단위 동기화 → 주 백업 저장소
  - dpms 백엔드 + DB 서버

---

## 2️⃣ 문제 요약

1. 로컬 디스크만 의존 → 포맷/삭제 사고 시 복구 불가  
2. NAS 논의 → 성능 저하, 단일 장애점, 비용/도입 기간 문제  
3. Git 도입 어려움 → 공용계정, 사용자 미숙련  
4. NFS 성능 문제 → 단순 파일 백업만 가능, 프로그램 실행 불가  
5. 리눅스 서버 4TB → 실행 안정적이지만 용량 부족

---

## 3️⃣ 제안 백업/버전 관리 절차

### 3.1 운영 원칙

- **실행/빌드/작업** → 장비 로컬 디스크 (I/O 성능 유지)  
- **백업/버전 관리** → dpms 증분 백업 + 윈도우 서버 저장  
- **임시/테스트 공유 공간** → 리눅스 4TB 서버 활용 (최근 데이터만, 오래된 데이터는 윈도우 서버로 이동)  
- **이력 관리** → dpms DB/hashcode 기반 버전 추적

### 3.2 백업 체계

| 구분          | 방법               | 대상               | 특징 |
|---------------|------------------|------------------|------|
| 1차 백업      | dpms 클라이언트   | 장비 12대         | 퇴근 전 수동 실행 or Makefile 자동 실행 가능 |
| 2차 저장      | dpms 서버 → 윈도우 서버 | RAID5 80TB ×2 HA  | 증분 백업, hashcode + DB 관리, 1시간 단위 동기화 |
| 임시 공유     | 리눅스 4TB       | 각 장비 마운트    | 실행/빌드 용도, 용량 한정 |

- dpms는 **metadata 기반 증분 전송** → tar + curl로 변경/추가 파일만 전송  
- 서버에서 **압축 + hashcode DB 저장** → Git과 유사한 버전 관리 기능 제공  
- 장비에서 바로 실행 가능한 **자동화 스크립트 포함** → 사용자 부담 최소화  

### 3.3 데이터 유형별 관리

| 데이터                 | 관리 방법             | 비고 |
|------------------------|--------------------|------|
| pgm / 빌드 스크립트    | dpms + Git 병행 가능  | 장기적으로 Git 도입 가능, 현재는 dpms로 이력 추적 |
| 결과 파일 (대용량)     | dpms 증분 백업       | Git 불필요, tar+curl로 효율적 |

### 3.4 장애 대응

1. **장비 로컬 디스크 포맷 사고** → dpms 서버에서 특정 시점 복원 가능  
2. **윈도우 서버 장애** → HA 구성으로 자동 전환, 1시간 동기화 기준 데이터 일부 손실 가능  
3. **네트워크 장애** → dpms 클라이언트에서 로컬 queue 저장 후 재전송 가능  

---

## 4️⃣ 권장 최적 플로우

1. **빌드 전/후 dpms 실행**
   - Makefile 통합 → 빌드 시 변경점 자동 전송

2. **장비/리눅스 서버 → 윈도우 서버 증분 백업**
   - Cron 하루 1회, 변경분만 전송
   - dpms hashcode + DB 기록

3. **윈도우 서버 HA + RAID5, DB replication**
   - 1시간 단위 동기화 → 서버 단일 장애 대비

4. **모니터링 + 알람 + 복구 시나리오**
   - 백업 성공/실패 로그, hashcode 무결성 확인

5. **장기 데이터 아카이브**
   - 오래된 결과 파일 → 압축/이관 → 스토리지 효율 확보

---

## 5️⃣ 추가 고려/보완 사항

| 구분                 | 보완 사항                                 | 이유 / 장점 |
|----------------------|------------------------------------------|-------------|
| 백업 주기             | Cron 1회 → 필요 시 장비별/파일별 증분 주기 조정 | 하루치 데이터 손실 가능성 최소화 |
| 복구 시나리오 문서화   | 각 사고 유형별 복구 절차 문서화           | 포맷, 장비 고장, 서버 HA 전환 등 대응력 확보 |
| 네트워크 장애 대비     | dpms 클라이언트 로컬 queue 기능 활성화    | 서버 접속 불가 시 재전송 가능 |
| 용량 관리             | 리눅스 4TB → 스테이징 용도 제한          | 실행 안정성과 용량 확보 |
| 모니터링/알람          | dpms 백업 성공/실패 알람 및 서버 상태 모니터링 | 장애 조기 감지, 운영 신뢰성 향상 |
| 데이터 무결성 확인     | dpms hashcode 기반 주기적 검증           | 전송/저장 과정 손상 확인 가능 |
| 보안/접근제어          | 장비별/사용자별 최소 권한, dpms 통신 암호화 | 데이터 유출 및 권한 문제 방지 |
| 장기 보관 정책         | 과거 데이터 보관 기간, 압축/아카이브 정책 수립 | 스토리지 효율, 법적/품질 요구사항 충족 |

---

## 6️⃣ 결론

- NAS 도입 없이도 현재 인프라만으로 **안정적 백업/복구 체계** 구축 가능  
- dpms + Cron + 윈도우 서버 HA 조합으로 사고 복구, 증분 백업, 버전 관리 모두 대응  
- 추가 보완: 모니터링, 용량 관리, 무결성 검증, 복구 시나리오 문서화  
- 장기적으로 pgm/빌드 스크립트는 Git 관리 가능 (사용자 교육 및 계정 정책 필요)

# 반도체 장비별 백업/형상 관리 현황 비교

| 구분        | 장비 수 | 저장 위치 / 구조                  | 백업 체계                           | 용량 / 유지 기간          | DPMS 사용 여부 | 특징 / 비고 |
|------------|--------|---------------------------------|-----------------------------------|-------------------------|---------------|------------|
| **HBM**     | 12대   | 로컬 디스크 2개 (1→2 하루 1회)  | Cron 하루 1회 → 윈도우 서버 80TB RAID5, HA / Makefile → DPMS 자동/수동 실행 | 로컬 300GB/장비, 서버 80TB | ○             | 로컬 디스크 포맷 사고 위험 존재, DPMS로 build 시점 변경/추가 파일 증분 관리, HA/DB replication 구성 예정 |
| **DRAM**    | 50대   | NAS 마운트 (공유 경로)           | NAS 내 솔루션 하루 1회, 30일간 유지 | NAS 30TB 이중화          | ○             | NAS 상에서 안정적 백업, DPMS로 형상 관리 가능, 장비별 I/O는 NAS 성능 영향 받을 수 있음 |
| **Validation** | 20대 | 장비 로컬 (PGM + 결과)           | 사용자 수동 백업                     | 장비 로컬                | ○             | 백업 주기 없음, 사용자 의존, DPMS 적용으로 증분 백업 가능, 중앙 서버/HA 없음 |




# 반도체 장비별 백업/형상 관리 현황 및 리눅스 서버 확장 시 고려 구성

## 1. 현황 비교 (HBM / DRAM / Validation)

| 구분          | 장비 수 | 저장 위치 / 구조                        | 백업 체계                                    | 용량 / 유지 기간          | DPMS 사용 여부 | 특징 / 비고 |
|---------------|--------|--------------------------------------|--------------------------------------------|-------------------------|---------------|------------|
| **HBM**       | 12대   | 로컬 디스크 2개 (1→2 하루 1회)         | Cron 하루 1회 → 윈도우 서버 80TB RAID5, HA / Makefile → DPMS 자동/수동 | 로컬 300GB/장비, 서버 80TB | ○             | 로컬 디스크 포맷 사고 위험 존재, DPMS로 build 시점 변경/추가 파일 증분 관리, HA/DB replication 구성 예정 |
| **DRAM**      | 50대   | NAS 마운트 (공유 경로)                  | NAS 내 솔루션 하루 1회, 30일간 유지        | NAS 30TB 이중화          | ○             | NAS 상에서 안정적 백업, DPMS로 형상 관리 가능, 장비별 I/O는 NAS 성능 영향 |
| **Validation**| 20대   | 장비 로컬 (PGM + 결과)                 | 사용자 수동 백업                             | 장비 로컬                | ○             | 백업 주기 없음, 사용자 의존, DPMS 적용으로 증분 백업 가능, 중앙 서버/HA 없음 |

---

## 2. 현재 HBM 최적 구성 (보유 장비 기반)

1. **백업 절차**
   - Cron → 장비 로컬/4TB 마운트 볼륨 → 윈도우 서버 80TB RAID5, 하루 1회
   - Makefile → DPMS 추가 → Build 시점 변경/추가 파일 증분 백업
   - 수동 실행 → Run-time 생성/수정 파일 백업 가능
2. **서버 구성**
   - 윈도우 서버 RAID5 + HA 이중화
   - DPMS DB replication 구성

---

## 3. 리눅스 서버 150TB 2대 추가 시 고려 구성

- **목적**: HBM/Validation 장비 백업과 형상 관리 안정성 강화, NAS 의존 최소화, 대용량 저장 지원
- **구성 제안**

| 항목                 | 구성 / 역할 |
|---------------------|------------|
| 리눅스 서버 150TB 2대 | - HBM/Validation 증분 백업 전용<br>- DPMS tar+curl 데이터 수신 및 압축, hash DB 저장<br>- 기존 4TB 리눅스 서버 역할 통합/대체 가능<br>- HA 구성 가능, 실시간 rsync/DRBD 적용 가능 |
| 윈도우 서버 80TB 2대 | - DRAM NAS 형상 관리 연계<br>- HBM 과거 데이터 및 검증용 reference backup 저장 가능<br>- 기존 RAID5 + HA 유지 |
| 백업 정책            | 1. Cron으로 장비 로컬 → 리눅스 서버 150TB 증분 백업<br>2. Makefile → DPMS 자동/수동 실행<br>3. DPMS DB replication / 리눅스 서버 간 동기화 |
| 추가 고려 사항        | - 장비별 접근 속도 최적화: HBM/Validation → 리눅스 서버, DRAM → NAS<br>- 서버 HA/replication 설계로 장애 대비<br>- DPMS와 연계하여 build 시점 + run-time 변경 파일 완전 관리<br>- 용량 관리: 150TB 서버로 12대 HBM + 20대 Validation 충분히 커버 가능 |

**장점**
- 로컬 디스크 의존 최소화 → 포맷 사고 방지
- DPMS 증분 관리 완전 활용 가능
- 대용량 저장으로 장기 보관 용이
- NAS/윈도우 서버 부하 분산 가능

**추가 권장**
- DPMS 클라이언트와 서버 간 전송 암호화 및 전송 로그 관리
- 리눅스 서버 증분 백업 스케줄링 최적화 (장비 idle 시간 활용)
- DPMS DB 모니터링 및 정기 백업 → HA 구성 시 장애 대비

